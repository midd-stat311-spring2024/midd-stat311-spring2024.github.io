[
  {
    "objectID": "weeks/hw/templates/hw2_r.html",
    "href": "weeks/hw/templates/hw2_r.html",
    "title": "STAT 311: Homework 2 (R)",
    "section": "",
    "text": "We want to answer the following question: How much does the prior distribution impact the posterior distribution of \\(\\theta\\)? Studying the impact of the prior distribution choice is called sensitivity analysis. More generally, sensitivity analysis occurs whenever we look at how varying our model assumptions impacts inference.\nWhen I arrived at Middlebury, I thought that the correct pronunciation of the school was “middle-BURRY”, but then a colleague told me that it is actually pronounced “middle-BERRY”. That being said, I’ve continued to hear both pronunciations around campus, and I think people just say “midd” to avoid the pronunciation issue altogether.\nLet \\(\\theta\\) be the true proportion of Middlebury students who pronounce the name as “middle-BURRY”. We will explore the posterior for \\(\\theta\\) under varying priors.\na) Choosing priors. Using the Beta distribution, construct and graph three possible prior distributions for \\(\\theta\\) by choosing different hyperparameters. For prior 1: make each value in the parameter space equally likely, for prior 2: make values in the center most likely, for prior 3: skew to the left.\nMake sure to state the hyperparameters for each prior, and try to pick Betas that are fairly different from each other. To learn more about a function, type ?command_name in the Console. e.g. ?dbeta\nBelow is a partially completed R chunk. Replace the underscores with relevant code, then change the eval = FALSE to eval = TRUE in the chunk header. You can run the code chunk by pressing the green triangle at the top-right of the chunk.\n\n# Sequence of parameter values (i.e. discretized parameter space)\np <- seq(0, 1, by = 0.01)\n\n# Prior 1\na1 <- ___\nb1 <- ___\nprior1 <- dbeta(x = p, shape1 = a1, shape2 = b1)\n\n# Prior 2\na2 <- ___\nb2 <- ___\nprior2 <- dbeta(x = p, shape1 = a2, shape2 = b2)\n\n# Prior 3\na3 <- ___\nb3 <- ___\nprior3 <- dbeta(____)\n\nplot(p, prior1, type=\"l\",ylab=\"Probability\", xlab=\"p\", col=\"green\", lwd=2, ylim=c(0,max(c(prior1, prior2, prior3))))\nlines(p,prior2,lty=2,col=\"blue\", lwd=2)\nlines(p,prior3,lty=3, col=\"purple\", lwd=2)\nlegend(\"topright\", legend=c(\"Prior 1\",\"Prior 2\", \"Prior 3\"),lty=1:3 , col=c(\"green\",\"blue\",\"purple\"))\n\nb) Collecting data. Ask 12 other Middlebury students (not in this class) to say the name of the college, and keep track of whether they pronounce it as “BURRY” (1) or “BERRY” (0). It’s important to not bias respondents by saying the name “Middlebury” yourself!\nSolution: enter the data here:\n\\(X_{1}\\):\n\\(X_{2}\\):\n\\(X_{3}\\):\n\\(X_{4}\\):\n\\(X_{5}\\):\n\\(X_{6}\\):\n\\(X_{7}\\):\n\\(X_{8}\\):\n\\(X_{9}\\):\n\\(X_{10}\\):\n\\(X_{11}\\):\n\\(X_{12}\\):\nc) Graphing posteriors (part 1). Using only the first three observations, compute and graph the three posterior distributions for the parameter based on your three prior distributions. Comment on any differences between the posteriors.\nAgain, replace the underscores with relevant code. Then change the eval = FALSE to eval = TRUE.\n\n# number of successes in first THREE observations\nx <- ___\n\n# number of observations under consideration\nn <- ___\n\n# posteriors\npost1_a <- dbeta(x=p, shape1 = ___, shape2 = ___)\npost2_a <- dbeta(x=p, shape1 = ___, shape2 = ___)\npost3_a <- dbeta(x=p, shape1 = ___, shape2 = ___)\n\n# plot\nplot(p, post1_a, type=\"l\",ylab=\"Probability\", xlab=\"p\", col=\"green\", lwd=2, ylim=c(0,max(c(post1_a, post2_a, post3_a))))\nlines(p,post2_a,lty=2,col=\"blue\", lwd=2)\nlines(p,post3_a,lty=3, col=\"purple\", lwd=2)\nlegend(\"topright\", legend=c(\"Prior 1\",\"Prior 2\", \"Prior 3\"),lty=1:3 , col=c(\"green\",\"blue\",\"purple\"))\n\nSolution:\nd) Graphing posteriors (part 2). Now using all of your observations, compute and graph the three posterior distributions for the parameter based on your three prior distributions. Comment on any differences between the posteriors.\nAgain, replace the underscores with relevant code. Then change the eval = FALSE to eval = TRUE.\n\nx2 <- ___\nn2 <- ___\n\n# posteriors\npost1_all <- dbeta(x=p, shape1 = ___, shape2 = ___)\npost2_all <- dbeta(x=p, shape1 = ___, shape2 = ___)\npost3_all <- dbeta(x=p, shape1 = ___, shape2 = ___)\n\nplot(p, post1_all, type=\"l\",ylab=\"Probability\", xlab=\"p\", col=\"green\", lwd=2, ylim=c(0,max(c(post1_all, post2_all, post3_all))))\nlines(p,post2_all,lty=2,col=\"blue\", lwd=2)\nlines(p,post3_all,lty=3, col=\"purple\", lwd=2)\nlegend(\"topright\", legend=c(\"Prior 1\",\"Prior 2\", \"Prior 3\"),lty=1:3 , col=c(\"green\",\"blue\",\"purple\"))\n\nSolution:\ne) Which prior distribution showed greater agreement with the data? Explain why it is important to select the prior distribution before collecting the data.\nSolution:"
  },
  {
    "objectID": "weeks/hw/templates/hw2_r.html#question-2-assigned-for-friday-223",
    "href": "weeks/hw/templates/hw2_r.html#question-2-assigned-for-friday-223",
    "title": "STAT 311: Homework 2 (R)",
    "section": "Question 2 (assigned for Friday 2/23)",
    "text": "Question 2 (assigned for Friday 2/23)\nWe saw in class that the Normal distribution is the conjugate prior in the case of data from a Normal unknown mean and known variance. However, sometimes a conjugate prior does not accurately reflect prior knowledge and so we need to use a different prior. Suppose \\(X_{1}, \\ldots, X_{n} | \\theta \\sim N(\\theta, 1)\\) (conditionally independent) and let the prior for \\(\\theta\\) be \\[p(\\theta) = c e^{-|\\theta|}, \\qquad \\theta \\in \\mathbb{R}\\]\nfor some constant \\(c > 0\\) which is a normalizing constant that ensures the PDF integrates to 1.\na) Fill in the rest of the code below to write an R function for the kernel of the prior distribution of \\(\\theta\\) (i.e. your function takes the argument theta as input and should return a value equal to \\(e^{-|\\theta|}\\)). Then use the integrate() function to find the value of \\(c\\), which will be stored as the variable c in the code below. Set eval = TRUE when finished.\n(Note: the parameter space of \\(\\theta\\) is the entire real line, so the bounds for the integrate() function are -Inf and Inf).\n\nprior_kernel <- function(theta){\n  # write your code for the kernel of the prior here\n}\n\nval <- integrate(____)\n\n# normalizing constant for prior\nc <- ___\nc\n\nb) Suppose we observe \\(n = 6\\) data points: \\(X_{1} = 5, X_{2} = 4.1, X_{3} = 3.9, X_{4} = 6, X_{5} = 5.5, X_{6} = 4.8\\).\nThe function posterior_kernel() below takes in a value theta as input, and should return the posterior evaluated at theta, up to proportionality. Fill out the rest of the function. In the code below, sum_of_squares is manually calculating \\(\\sum_{i=1}^{n} (x_{i} - \\theta)^2\\) by iterating over each observation \\(i\\).\nIn particular, lh should be calculated using sum_of_squares.\nThen use your function and integrate() to find the normalizing constant of the posterior distribution of \\(\\theta\\) given the data. Call this normalizing constant c_post, and report its value. Set eval = TRUE when finished.\n\nposterior_kernel <- function(theta){\n  sum_of_squares <- 0\n  dat <- c(5, 4.1, 3.9, 6, 5.5, 4.8)\n  for(i in 1:length(dat)){\n    square_i <- (dat[i] - theta)^2\n    sum_of_squares <- sum_of_squares + square_i\n  }\n  lh <- ___ # likelihood up to proportionality \n  to_return <- ___ # this should be (likelihood) x (un-normalized prior evaluated at theta)\n  return(to_return)\n}\n\n# integrate\n\n# store/save normalizing constant\n\nc): Graph the exact prior and posterior distributions of \\(\\theta\\) on the same graph for the provided values of \\(\\theta \\in [-3, 10]\\). Here, “exact” means including the normalizing constants such that the distributions are proper PDFs.\n\ntheta_vals <- seq(-3, 10, 0.1)\n\n## plot posterior \nplot(____, ____, type=\"l\",ylab=\"Probability\", xlab=\"p\", col=\"green\", lwd=2)\n\n# add prior\nlines(____, ____,lty=2,col=\"blue\", lwd=2)\nlegend(\"topright\", legend=c(\"posterior\", \"prior\"),lty=1:3 , col=c(\"green\",\"blue\"))\n\nd): Use the integrate() function to find the Bayes estimate for \\(\\theta\\) under squared loss (you will need to write a new R function). Compare the Bayes estimate to the observed sample mean. (Recall that we can find the mean value of a vector of values using the mean() function.)\n\n# write new R function\n\n# obtain Bayes estimate\n\n# obtain sample mean\n\nSolution:"
  },
  {
    "objectID": "weeks/week-02.html",
    "href": "weeks/week-02.html",
    "title": "Week 02",
    "section": "",
    "text": "Tuesday (02/20)\n\nTopics\n\nMore with posterior distributions\n\nDaily assignment\n\nNone for today!\nPlease bring a laptop with R ready to go\n\nClass activity\n\nPrior and posterior practice problems\n\n\n\nThursday (02/22)\n\nTopics\n\nConjugate priors\nImproper priors\n\nDaily assignment\nTo be completed and submitted to Canvas by 8:45am\n\nDaily assignment\n\nClass activity\n\nComputing posteriors in R\n\n.Rmd template:  .Rmd \nFilled-in code: computing posteriors in R\n\nConjugate prior practice problem\n\n\n\nFriday (02/23)\n\nTopics\n\nBayes estimators\n\nDaily assignment\nTo be completed and submitted to Canvas by 8:45am\n\nDaily assignment\n\nClass activity\n\nBayes estimator under absolute loss proof\nBayes estimator practice problem"
  },
  {
    "objectID": "weeks/week-03.html",
    "href": "weeks/week-03.html",
    "title": "Week 03",
    "section": "",
    "text": "Tuesday (02/27)\n\nTopics\n\nMethod of Maximum Likelihood\n\nDaily assignment\n\nDaily assignment\n\nClass activity\n\nMLE practice problems\n\n\n\nThursday (02/29)\n\nTopics\n\nMLEs (cont.)\n\nDaily assignment\n\nNothing official, though I suggest re-reading Section 7.5 and digging out your multivariable calculus notes to remember what a Hessian matrix is!\n\nClass activity\n\nMore MLE practice problems\n\n\n\nFriday (03/01)\n\nTopics\n\nProperties of MLEs\nConsistency\n\nDaily assignment\nTo be completed and submitted to Canvas by 8:45am\n\nDaily assignment\n\nClass activity\n\nInvariance and consistency practice problems"
  },
  {
    "objectID": "weeks/week-01.html",
    "href": "weeks/week-01.html",
    "title": "Week 01",
    "section": "",
    "text": "Tuesday (02/13)\n\nTopics\n\nWelcome!\nCourse logistics\n\nDaily assignment\n\nReview syllabus\nOpen (or re-download) RStudio and make sure everything is working\n\nClass activity\n\nIntroduction to inference\n\n\n\nThursday (02/15)\n\nTopics\n\nCore terminology\n\nDaily assignment\nTo be completed and submitted to Canvas by 8:45am\n\nDaily assignment\n\n\n\nFriday (02/16)\n\nTopics\n\nPrior distributions\nPosterior distributions\n\nDaily assignment\nTo be completed and submitted to Canvas by 8:45am\n\nDaily assignment"
  },
  {
    "objectID": "weeks/class/posteriors_in_R_solns.html",
    "href": "weeks/class/posteriors_in_R_solns.html",
    "title": "Obtaining posteriors in R",
    "section": "",
    "text": "Suppose \\(X_{1},\\ldots, X_{5}\\) form a random sample from a \\(\\text{Geometric}(\\theta)\\) distribution, where \\(\\theta \\in (0,1)\\) is the unknown probability of success. We can use a Beta prior for \\(\\theta\\): \\(\\theta \\sim \\text{Beta}(a, b)\\)."
  },
  {
    "objectID": "weeks/class/posteriors_in_R_solns.html#visualize-prior",
    "href": "weeks/class/posteriors_in_R_solns.html#visualize-prior",
    "title": "Obtaining posteriors in R",
    "section": "Visualize prior",
    "text": "Visualize prior\nSuppose I pick hyperparameter values \\(a = 1\\) and \\(b = 4\\). What does this particular distribution look like?\n\ntheta_vals <- seq(0.01, 0.99, by = 0.01)\na <- 1\nb <- 4\n\nprior_vals <- dbeta(theta_vals, shape1 = a, shape2 = b)\n\nplot(theta_vals, prior_vals, type = \"l\", main = \"Prior\")"
  },
  {
    "objectID": "weeks/class/posteriors_in_R_solns.html#verify-prior-is-a-proper-pdf",
    "href": "weeks/class/posteriors_in_R_solns.html#verify-prior-is-a-proper-pdf",
    "title": "Obtaining posteriors in R",
    "section": "Verify prior is a proper PDF",
    "text": "Verify prior is a proper PDF\nWe know the prior is a valid PDF because the Beta distribution is a well known distribution. But let’s double check by using R to verify that that the PDF integrates to 1.\nWe will write a function called beta_prior that takes in an argument theta as input, and should return the Beta density evaluated at that value of theta and our choice of hyperparameters. Then, we will integrate() our function over its support to see if the PDF in fact integrates to 1.\n\nbeta_prior <- function(theta){\n  to_return <- dbeta(theta, a, b)\n  return(to_return)\n}\n\nintegrate(beta_prior, lower = 0, upper = 1)\n\n1 with absolute error < 1.1e-14\n\n# to obtain the number by itself\nintegrate(beta_prior, lower = 0, upper = 1)$value\n\n[1] 1"
  },
  {
    "objectID": "weeks/class/posteriors_in_R_solns.html#obtain-and-plot-posterior",
    "href": "weeks/class/posteriors_in_R_solns.html#obtain-and-plot-posterior",
    "title": "Obtaining posteriors in R",
    "section": "Obtain and plot posterior",
    "text": "Obtain and plot posterior\nNow suppose we observe \\(X = (3, 7, 3, 5, 6)\\).\n\nx <- c(3,7,3,5,6)\nn <- length(x)\n\n\nFrom known results\nWhat is the posterior for \\(\\theta\\) under our \\(\\text{Beta}(1,4)\\) prior given the observed data? From class work, we know:\n\\[\\theta | \\mathbf{x} \\sim \\text{Beta}(1 + n, 4 + \\sum_{i=1}^{n} x_{i})\\]\n\npost_vals <- dbeta(theta_vals, a + n, b + sum(x))\nplot(theta_vals, post_vals, type = \"l\", main = \"Using Beta posterior\")\n\n\n\n\n\n\nBy obtaining normalizing constant\nSuppose we didn’t know that the posterior is Beta. However, we know that the posterior is always proportional to the likelihood times prior; all we lack is the normalizing constant. So let’s write a function that evaluates the kernel of the posterior (i.e. the posterior up to proportionality), then use it to find the normalizing constant.\n\npost_kernel <- function(theta){\n  # obtain likelihood for theta under Geometric (up to proportionality)\n  lh <- 1\n  for(i in 1:n){\n    lh <- lh * ((1-theta)^x[i]) * theta\n  }\n  # prior (up to proportionality)\n  prior <- theta^(a-1) * (1-theta)^(b-1)\n  to_return <- lh * prior\n  return(to_return)\n}\n\nval <- integrate(post_kernel, 0, 1)$val\nval\n\n[1] 1.504799e-07\n\n# We can even verify this, since we calculated the normalizing constant analytically!\n(gamma(a + n) * gamma(b + sum(x)))/gamma(a + b + n + sum(x))\n\n[1] 1.504799e-07\n\npost_vals2 <- post_kernel(theta_vals) / val\n\nWe can see that 1) the posterior obtained through normalization agrees with the analytic result, and 2) the unnormalized posterior has exactly the same shape as the normalized:\n\npar(mfrow = c(1,2))\nplot(theta_vals, post_vals2, type = \"l\", main = \"Using normalization\")\nplot(theta_vals, post_kernel(theta_vals), type = \"l\", main = \"Unnormalized\",\n     ylab = \"density\")\n\n\n\n\nLastly, we can visualize the prior and posterior on the same plot by first creating one plot, then using the lines() function to add additional lines to the original plot.\n\nplot(theta_vals, post_vals2, type = \"l\", main = \"Prior and posterior\",\n     ylab = \"density\")\nlines(theta_vals, prior_vals, type = \"l\", col = \"orange\")"
  },
  {
    "objectID": "weeks/class/templates/posteriors_in_R.html",
    "href": "weeks/class/templates/posteriors_in_R.html",
    "title": "Obtaining posteriors in R",
    "section": "",
    "text": "Suppose \\(X_{1},\\ldots, X_{5}\\) form a random sample from a \\(\\text{Geometric}(\\theta)\\) distribution, where \\(\\theta \\in (0,1)\\) is the unknown probability of success. We can use a Beta prior for \\(\\theta\\): \\(\\theta \\sim \\text{Beta}(a, b)\\)."
  },
  {
    "objectID": "weeks/class/templates/posteriors_in_R.html#visualize-prior",
    "href": "weeks/class/templates/posteriors_in_R.html#visualize-prior",
    "title": "Obtaining posteriors in R",
    "section": "Visualize prior",
    "text": "Visualize prior\nSuppose I pick hyperparameter values \\(a = 1\\) and \\(b = 4\\). What does this particular distribution look like?\n\n# create vector of possible theta values\ntheta_vals <- seq(0.01, 0.99, by = 0.01)\n\n# set prior hyperparameters\n\n\n# evaluate the prior at each of the elements of theta_vals\nprior_vals <- \n\n# plot prior dist"
  },
  {
    "objectID": "weeks/class/templates/posteriors_in_R.html#verify-prior-is-a-proper-pdf",
    "href": "weeks/class/templates/posteriors_in_R.html#verify-prior-is-a-proper-pdf",
    "title": "Obtaining posteriors in R",
    "section": "Verify prior is a proper PDF",
    "text": "Verify prior is a proper PDF\nWe know the prior is a valid PDF because the Beta distribution is a well known distribution. But let’s double check by using R to verify that that the PDF integrates to 1.\nWe will write a function called beta_prior that takes in an argument theta as input, and should return the Beta density evaluated at that value of theta and our choice of hyperparameters. Then, we will integrate() our function over its support to see if the PDF in fact integrates to 1.\n\n# write function that evaluates our prior\nbeta_prior <- function(theta){\n  \n}\n\n# verify that our prior integrates to 1"
  },
  {
    "objectID": "weeks/class/templates/posteriors_in_R.html#obtain-and-plot-posterior",
    "href": "weeks/class/templates/posteriors_in_R.html#obtain-and-plot-posterior",
    "title": "Obtaining posteriors in R",
    "section": "Obtain and plot posterior",
    "text": "Obtain and plot posterior\nNow suppose we observe \\(X = (3, 1, 2, 5, 6)\\).\n\ndat <- c(3,1,2,5,6)\nn <- length(dat)\n\n\nFrom known results\nWhat is the posterior for \\(\\theta\\) under our \\(\\text{Beta}(2,2)\\) prior given the observed data? From class work, we know:\n\\[\\theta | \\mathbf{x} \\sim\\]\n\n# create vector of posterior values\n\n# plot posterior\n\n\n\nBy obtaining normalizing constant\nSuppose we didn’t know that the posterior is a Beta. However, we know that the posterior is always proportional to the likelihood times prior! So all we lack is the normalizing constant. We can write a function that evaluates the kernel of the posterior (i.e. all the parts that include \\(\\theta\\)).\n\n# write function that evaluate the kernel of the posterior for a given theta value\n\n# integrate the function to obtain normalizing constant\n\n# obtain normalizing constant \n\n# plot posterior\n\nWe can see that the unnormalized posterior has exactly the same shape:\n\nplot(theta_vals, post_kernel(theta_vals), type = \"l\", main = \"Prior and posterior\",\n     ylab = \"density\")\n\nWe can visualize the prior and posterior on the same plot by first creating one plot, then using the lines() function to add additional lines to the original plot.\n\nplot(theta_vals, post_vals, type = \"l\", main = \"Prior and posterior\",\n     ylab = \"density\")\nlines(theta_vals, prior_vals, type = \"l\", col = \"orange\")"
  },
  {
    "objectID": "weeks/class/proof_bayes_est.html#theorem",
    "href": "weeks/class/proof_bayes_est.html#theorem",
    "title": "Bayes estimator under absolute loss",
    "section": "Theorem",
    "text": "Theorem\nUnder absolute loss \\(L(\\theta, a) = |\\theta - a|\\), a Bayes estimator for \\(\\theta\\) is any posterior median of \\(\\theta\\).\n\nThat is, a Bayes estimator is a function \\(\\delta(\\mathbf{X}) \\equiv m\\) such that \\(\\text{Pr}(\\theta \\leq m | \\mathbf{x}) \\geq \\frac{1}{2}\\) and \\(\\text{Pr}(\\theta \\geq m | \\mathbf{x}) \\geq \\frac{1}{2}\\).\nNote that when \\(\\theta\\) is continuous, there exists a single median."
  },
  {
    "objectID": "weeks/class/proof_bayes_est.html#proof-set-up",
    "href": "weeks/class/proof_bayes_est.html#proof-set-up",
    "title": "Bayes estimator under absolute loss",
    "section": "Proof set-up",
    "text": "Proof set-up\n\nNote that the absolute value function is not differentiable, so proving a maximum/minimum cannot rely on derivatives!\nAssume that \\(\\theta\\) is continuous, so that if \\(m\\) is the posterior median, then \\(\\text{Pr}(\\theta \\geq m | \\mathbf{x}) = \\frac{1}{2} = \\text{Pr}(\\theta \\leq m | \\mathbf{x})\\).\nLet \\(a\\) be any other estimator of \\(\\theta\\).\nWe will show that\n\\[\n\\mathbb{E}[L(\\theta , a) | \\mathbf{x}] - \\mathbb{E}[L(\\theta, m) | \\mathbf{x}]  \\geq 0\n\\]\nthus demonstrating that \\(m\\) minimizes the expected loss."
  },
  {
    "objectID": "weeks/class/proof_bayes_est.html#proof",
    "href": "weeks/class/proof_bayes_est.html#proof",
    "title": "Bayes estimator under absolute loss",
    "section": "Proof",
    "text": "Proof\nLet \\(m\\) be the posterior median, and suppose \\(a < m\\) is any other estimator.\n\\[\n\\begin{align}\n\\mathbb{E}[L(\\theta , a) | \\mathbf{x}] & - \\mathbb{E}[L(\\theta, m) | \\mathbf{x}] = \\int_{\\Omega} |\\theta - a| p(\\theta | \\mathbf{x}) d\\theta - \\int_{\\Omega} |\\theta - m| p(\\theta | \\mathbf{x})d\\theta \\\\\n&\\class{fragment}{= \\int_{\\Omega} \\left(|\\theta - a| - |\\theta - m|\\right) p(\\theta | \\mathbf{x})d\\theta } \\\\\n&\\class{fragment}{= \\int_{-\\infty}^{a} \\left(|\\theta - a| - |\\theta - m|\\right) p(\\theta | \\mathbf{x})d\\theta + \\int_{a}^{m} \\left(|\\theta - a| - |\\theta - m|\\right) p(\\theta | \\mathbf{x})d\\theta + \\int_{m}^{\\infty} \\left(|\\theta - a| - |\\theta - m|\\right) p(\\theta | \\mathbf{x})d\\theta }\\\\\n& \\class{fragment}{= \\int_{-\\infty}^{a} ((a-\\theta) - ( m - \\theta)) p(\\theta | \\mathbf{x}) d\\theta + \\int_{a}^{m} ((\\theta - a) - (m- \\theta)) p(\\theta | \\mathbf{x}) d\\theta  +\n\\int_{m}^{\\infty} ((\\theta - a) - (\\theta - m)) p(\\theta | \\mathbf{x}) d\\theta} \\\\\n&\\class{fragment}{= \\int_{-\\infty}^{a} (a - m)  p(\\theta | \\mathbf{x}) d\\theta + \\int_{a}^{m} (2\\theta - a- m)  p(\\theta | \\mathbf{x}) d\\theta + \\int_{m}^{\\infty} (m-a) p(\\theta | \\mathbf{x}) d\\theta} \\\\\n&\\class{fragment}{\\color{orange}{\\geq}  \\int_{-\\infty}^{a} (a - m)  p(\\theta | \\mathbf{x}) d\\theta + \\int_{a}^{m} (2\\color{orange}{a} - a- m)  p(\\theta | \\mathbf{x}) d\\theta + \\int_{m}^{\\infty} (m-a) p(\\theta | \\mathbf{x}) d\\theta} \\\\\n&\\class{fragment}{= (a-m)\\text{Pr}(\\theta \\leq a | \\mathbf{x}) + \\color{purple}{(a-m)\\text{Pr}(a  < \\theta \\leq m | \\mathbf{x}) }+ (m-a) \\text{Pr}(\\theta \\geq m | \\mathbf{x})} \\\\\n&\\class{fragment}{ = (a-m)\\text{Pr}(\\theta \\leq a | \\mathbf{x}) + \\color{purple}{(a-m) \\text{Pr}(\\theta \\leq m | \\mathbf{x})  - (a-m) \\text{Pr}(\\theta \\leq a | \\mathbf{x})} + (m-a) \\text{Pr}(\\theta \\geq m | \\mathbf{x}) } \\\\\n&\\class{fragment}{= (a-m) \\text{Pr}(\\theta \\leq m | \\mathbf{x}) +  (m-a) \\text{Pr}(\\theta \\geq m | \\mathbf{x})} \\\\\n&\\class{fragment}{= (a-m)\\left(\\frac{1}{2}\\right) + (m-a)\\left(\\frac{1}{2}\\right)} \\\\\n&\\class{fragment}{= (a-m)\\left(\\frac{1}{2}\\right) - (a-m)\\left(\\frac{1}{2}\\right)}\\\\\n&\\class{fragment}{= 0 \\qquad \\tiny{\\square} }\n\\end{align}\n\\]"
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "Homework",
    "section": "",
    "text": "Note: problem sets will be updated with new questions following each class. Completed homework should be submitted to Canvas unless otherwise noted.\n\nHomework 1 (due Tuesday, 2/20 at 11:59pm)\n\nProblems assigned for Friday 2/16 technically cover material from Section 7.2, but we haven’t done examples in class yet. You are welcome to work on these problems now, or wait until after class 2/20.\n\nHomework 2 (due Tuesday, 2/29 at 11:59pm)\n\nNote: associated R problems can be found and completed in the following .Rmd template:  .Rmd \n\nA rendered version for easier reading can be viewed here: HW 2: R (rendered)\n\n\nHomework 3 (due Tuesday, 3/05 at 11:59pm)\n\nNote: I swapped the order of problems 3 and 5. The version as of Thursday, 2/29 is correct.\n\nHomework 4 (due Tuesday, 3/12 at 11:59pm)\n\nNote: associated R problems can be completed in the following .Rmd template:  .Rmd \n\nA rendered version for easier reading can be viewed here: HW 4: R (rendered)\n\n\nHomework 5 (due Tuesday, 3/29 at 11:59pm)\nExtra credit opportunity about EM algorithm (due Monday, 4/01 at 11:59pm)\n\nImplement using the following .Rmd template:  .Rmd \n\nA rendered version for easier reading can be viewed here\n\n\nHomework 6 (due Wednesday, 4/10 at 11:59pm)\n\nNote the atypical due date!\nNote: associated R problems can be completed in the following .Rmd template:  .Rmd \n\nA rendered version for easier reading can be viewed here: HW 6: R (rendered)\n\n\nHomework 7 (due Tuesday, 4/16 at 11:59pm)\n\nThis homework will be be a bit shorter, but is R heavy"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Welcome to the website for Middlebury College’s Spring 2024 STAT 311. On this website you will find the course syllabus, schedule, and assignments. The website is frequently updated during the semester, so please make a habit of refreshing the page. The icon at the top right will link to the course Canvas where assignments should be submitted."
  },
  {
    "objectID": "index.html#major-announcements",
    "href": "index.html#major-announcements",
    "title": "Statistical Inference",
    "section": "Major Announcements",
    "text": "Major Announcements\n\nMidterm 1 (non-R) revisions (due physically in class Tuesday, 04/09):\n\nFor partial credit back, you are welcome to revise any and all parts in which you received less than 50%. My expectation is that for every part you revise, you should completely re-write a new solution.\n\nThis may also require you to revise previous parts of problems in which you received more then 50% credit but not full 100%. If this is the case, please include this work in your revision as well.\n\nTempting as it may be, try not work with others/ask each other how you did certain parts.\n\nFinal project announced. Take a look at the description!\nNew office hours schedule as follows:\n\nMondays 2:00-4:00pm (starting on 04/15)\nTuesdays 10:45am-12:00pm (as usual)\nHowever, I’m travelling this coming Monday 4/08, so I will extend due date of Homework 6 to Wednesday 04/10 and have the following make-up office hours:\n\nWednesday 04/10 from 3:30-4:30pm\n\n\n\n\nRequired textbook: Probability and Statistics, 4th edition by DeGroot and Schervish."
  },
  {
    "objectID": "index.html#course-details",
    "href": "index.html#course-details",
    "title": "Statistical Inference",
    "section": "Course Details",
    "text": "Course Details\nInstructor: Becky Tang\n\nOffice: WNS 214\nEmail: btang@middlebury.edu\n\nMeeting times: TRF 9:45-10:35am in WNS 011\nOffice hours: M 2:00-4:00pm, T 10:45am-12pm\nSyllabus (most recent update: 02/15/24 to reflect updated office hours)\nDistribution sheet (from MATH/STAT 310)"
  },
  {
    "objectID": "exam.html",
    "href": "exam.html",
    "title": "Exams",
    "section": "",
    "text": "Midterm 1 will be take-home. The midterm will be created as a Quiz in Canvas, and will be available/live starting at 5:00pm on Friday, March 29 through 11:59pm on Monday, April 1. You must upload your completed midterm to Canvas.\nThe first midterm will cover content up to material on 03/14 in Week 5. There will be some questions that ask you to use R.\nIf applicable, please send me a Letter of Accommodation before Spring Break!\n\n\nThe exam will be broken down into two components: a component that requires R, and a component that does not require R (beyond using it as a calculator). These will appear as separate quizzes on Canvas. This format is intended to allow you to take a break between components if you’d like one. Your overall score for Midterm 1 will calculated as: (points on non-R + points on R)/(total possible points).\n\n\n\nYou will have 3 hours starting at the time that you open the non-R portion of the midterm to complete it. You must upload your solutions to Canvas by the end of those 3 hours. It is your responsibility to ensure that your Midterm is uploaded correctly to Canvas. Please double-check that you have indeed submitted your midterm.\nYour solutions should be neatly written or typed. If you scan your handwritten solutions, please be sure to review the legibility of your scan before uploading to Canvas.\n\n\n\n\n\nYou will have 1 hour starting at the time that you open the R portion of the midterm to complete it. You must upload your solutions to Canvas by the end of the hour. You must submit a knitted PDF/HTML document, and not the original .Rmd file. It is your responsibility to ensure that your Midterm is uploaded correctly to Canvas. Please double-check that you have indeed submitted your midterm.\nIf you are unable to knit due to bugs in your code, set eval = FALSE in the R chunk headers so the document will knit. That way, I can still see your code.\n\n\n\n\nIn both parts of the midterm, you will be required to write and/or type the honor code statement: “I  have neither given nor received unauthorized aid on this assignment.” See the following Resources subsection about what is considered unauthorized aid. Any submissions found in violation of this honor code will receive an automatic 0.\n\n\n\n\n\nThis midterm is open-book with respect to material shared/distributed in this MATH/STAT 311 course, but not open-people. The only person you are allowed to talk to about the midterm while it is “live” (available to the class) is Prof. Tang. This means that you should not ask someone if they’ve taken the midterm, how it went, etc. until after the official due date.\nYou may use any notes you’ve taken for this class and your MATH/STAT 310 course, your work on previous assignments/practice problems and its associated feedback, my recorded videos, and DeGroot and Schervish’s textbook. For problems asking you to use R, remember that you may reference any of R help file by typing ?functionname.\nYou may also use WolframAlpha, Maple, Mathematica, Desmos, or Symbolab to assist with calculating integrals, derivatives, or algebraic simplification. If you use technology to assist calculating, please clearly reference the site you used to assist you, and write down the explicit expression/code you input.\nYou may not use any other resources other than those listed above. That means no open internet/Google search or ChatGPT. If you have questions about whether a resource can be used, you are welcome to e-mail me.\nI will be regularly checking my e-mail from 8am-8pm while the midterm is live, but there might be delays of up to an hour in my responses.\n\n\n\n\n\nThe best preparation you can do for the exam is to organize your notes and/or homework to make finding information and examples as quick and efficient as possible. Beyond that, you should attempt to accurately assess what topics you have mastered and which you need to practice more. A good starting point is to review the list of objectives on each daily assignment. Another way to prepare is to create your own study guide with summaries of the important concepts, along with example problems you’ve designed and solved. As you study, it would be a good idea to compile lists of questions that you might have for Prof. Tang/the class.\nI encourage you to re-visit and re-do previous homework problems and practice problems. There are many practice problems that we did not have time to get to. Solutions to selected problems will be made available via Canvas over Spring Break.\nMake sure you feel comfortable creating new code chunks in an .Rmd file. You will be provided a starter template for this midterm, but I might not explicitly create code chunks for you ahead of time.\nExam problems will be comparable in difficulty to those assigned for homework. Some exam questions may be similar to problems you have seen before, while others will require you to synthesize your knowledge in new ways.\n\n\n\n\nFor extra practice, additional review problems will be made available below towards the end of Spring Break. While these questions are representative of the typical scope and difficulty of individual exam questions, this review is not comprehensive, nor does it necessarily represent the expected amount of time for it will take for you to complete the exam.\n\nMidterm 1 review problems"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "A high-level overview of the semester’s topics are presented here. Please select a week for a specific day’s assignments and materials.\n\nWeek 01\n\nWelcome!\nPrior and posterior distributions\n\n\n\nWeek 02\n\nConjugate priors\nBayes estimators\n\n\n\nWeek 03\n\nMaximum likelihood estimation\n\n\n\nWeek 04\n\nMethod of moments\nBias, variance, MSE\n\n\n\nWeek 05\n\nFisher information\nCramer-Rao Lower Bound\n\n\n\nSPRING BREAK\n\n\nWeek 06\n\n\\(\\chi^2\\) and \\(t\\) distributions\nJoint distribution of sample mean and variance\n\n\n\nWeek 07\n\nConfidence intervals\n\n\n\nWeek 08\n\nBootstrapping\n\n\n\nWeek 09\n\nHypothesis testing\np-values"
  },
  {
    "objectID": "weeks/week-04.html",
    "href": "weeks/week-04.html",
    "title": "Week 04",
    "section": "",
    "text": "Tuesday (03/05)\n\nTopics\n\nMethod of Moments\n\nDaily assignment\n\nDaily assignment\nPlease bring a laptop to class tomorrow!\n\nClass activity\n\nMethod of Moments simulation\nMethod of Moments practice problems\n\n\n\nThursday (03/07)\n\nTopics\n\nMean squared error of an estimator\nBias of an estimator\n\nDaily assignment\n\nDaily assignment\n\nClass activity\n\nBias practice problems\n\n\n\nFriday (03/08)\n\nTopics\n\nBias-variance trade-off\n\nDaily assignment\n\nNothing official, but review notes from yesterday.\n\nClass activity\n\nMSE practice problems"
  },
  {
    "objectID": "weeks/week-05.html",
    "href": "weeks/week-05.html",
    "title": "Week 05",
    "section": "",
    "text": "Tuesday (03/12)\n\nTopics\n\nFisher information\n\nDaily assignment\n\nDaily assignment\n\nClass activity\n\nFisher information practice problems\n\n\n\nThursday (03/14)\n\nMaterial through today is fair game for Midterm 1!\nTopics\n\nAsymptotic distribution of MLE\nCRLB\n\nDaily assignment\n\nDaily assignment\n\nClass activity\n\nCRLB practice problems\n\n\n\nFriday (03/15)\n\nTopics\n\nEM algorithm\n\nDaily assignment\n\nNone!\n\nClass activity\n\nEM algorithm notes (not filled in) and (filled in)\nExample\n\nDownloadable .Rmd template  .Rmd  and data  .Rda \nImplemented example"
  },
  {
    "objectID": "weeks/class/mark_recapture.html",
    "href": "weeks/class/mark_recapture.html",
    "title": "Method of Moments: Mark-Recapture Simulation",
    "section": "",
    "text": "Set-up\nRecall the following scenario: we would like estimate the number of individuals \\(N\\) of a species in a particular location. The mark-recapture sampling scheme proceeds as follows:\n\nSample \\(r\\) individuals from the population and mark/tag them all. Release the individuals and wait some time.\nAfter waiting, take a second sample of \\(m\\) individuals from the population. Count how many of the \\(m\\) are marked.\n\nDefining \\(X\\) as the number of individuals in the second sample who were tagged, we obtained the following method of moments estimator: \\(\\hat{N}_{MM} = \\frac{rm}{X}\\).\n\n\nSimulation\nLet’s simulate data to see how well this estimator performs. Note that in simulating data, I get to choose/know the true value of \\(N\\).\n\n# set true value\nN_true <- 2000\n\nNow, I will determine how many to take in the first sample. Then I create a vector of 1’s and 0’s called marked, where I have \\(r\\) 1’s representing all the tagged individuals, and the remaining \\(N-r\\) 0’s representing all the untagged.\n\n# number in first sample\nr <- 200\n\n# make vector of \"marked\" and \"unmarked\" individuals after initial sample\nmarked <- c(rep(1, r), rep(0, N_true -r))\n\nWe write a function that performs one iteration of the second sample: sampling \\(m\\) individuals without replacement from the population again, and counting how many were marked.\n\n# write function to simulate second sample\nmark_recapture <- function(marked, m){\n  samp2 <- sample(marked, size = m, replace = F)\n  x <- sum(samp2)\n  return(x)\n}\n\nNow we actually simulate! I can choose how many simulations to run. Then I use the replicate() function to repeatedly perform the simulations. The outcome all the replicates gets stored in a vector of length n_sims called R, which I use to obtain the method of moments estimates over repeated samples.\n\n# choose number of simulations\nn_sims <- 10000\n# number in second sample\nm <- 300\n\n# simulate!\nR <- replicate(n_sims, mark_recapture(marked, m))\n\n# calculate MoM estimate\nN_hat_vec <- r * m / R\n\n# plot\nlibrary(tidyverse)\ndata.frame(N_hat = N_hat_vec) %>%\n  ggplot(., aes(x = N_hat)) +\n  geom_histogram() +\n  geom_vline(xintercept = N_true, col = \"red\", linetype = \"dashed\")"
  },
  {
    "objectID": "weeks/hw/templates/hw4_r.html",
    "href": "weeks/hw/templates/hw4_r.html",
    "title": "STAT 311: Problem Set 4 (R)",
    "section": "",
    "text": "BEFORE SUBMITTING, CHECK YOUR KNITTED FILE. Did you provide written answers where you see Solution? Are all the results showing? If not, be sure to set eval = TRUE in the chunk header. Submissions where the code is not evaluated will not receive full credit."
  },
  {
    "objectID": "weeks/week-07.html",
    "href": "weeks/week-07.html",
    "title": "Week 07",
    "section": "",
    "text": "Tuesday (04/02)\n\nTopics\n\n\\(t\\) distribution (cont.)\nIntroduce final project\n\nDaily assignment\n\nNone\n\nClass activity\n\n\\(t\\) distribution practice problem\n\n\n\nThursday (04/04)\n\nTopics\n\nConfidence intervals\n\nDaily assignment\n\nDaily assignment\n\n\n\nFriday (04/05)\n\nTopics\n\nConfidence intervals (cont.)\n\nDaily assignment\n\nDaily assignment\n\nClass activity\n\nConfidence interval practice problems"
  },
  {
    "objectID": "weeks/week-06.html",
    "href": "weeks/week-06.html",
    "title": "Week 06",
    "section": "",
    "text": "No problem set this week to give space to study for the midterm! However, there are still daily assignments.\n\nTuesday (03/26)\n\nTopics\n\nSampling distribution of a statistic\nChi-squared distribution\n\nDaily assignment\n\nDaily assignment\n\nClass activity\n\nMGF refresher practice problems\n\n\n\nThursday (03/28)\n\nTopics\n\nJoint dist. of sample mean and variance\n\nDaily assignment\n\nDaily assignment\n\nClass activity\n\nCovariance and Normal distribution properties refresher\n\n\n\nFriday (03/29)\n\nTopics\n\nJoint dist. of sample mean and variance (cont.)\n\\(t\\) distribution\n\nDaily assignment\n\nDaily assignment\n\nClass activity\n\nSampling dist. of Normal variance estimators practice problems\n\nMidterm 1 live at 5:00pm"
  },
  {
    "objectID": "weeks/class/EM.html",
    "href": "weeks/class/EM.html",
    "title": "EM: Mixture of Normals",
    "section": "",
    "text": "library(tidyverse)\nx <- readRDS(\"~/Downloads/normal_mixture_data.Rda\")\ndata.frame(x = x) %>%\n  ggplot(., aes(x= x))+\n  geom_histogram(bins = 15)"
  },
  {
    "objectID": "weeks/hw/templates/hw4_r.html#question-1",
    "href": "weeks/hw/templates/hw4_r.html#question-1",
    "title": "STAT 311: Problem Set 4 (R)",
    "section": "Question 1",
    "text": "Question 1\nWe will simulate some German Tank data, obtain estimates of the number of tanks, and compare the estimators. You may want to refer back to the Mark-Recapture code from 03/05.\n\nCreate three variables:\n\nn_sim for the number of simulations (set this to be at least 1000)\nN_true for the true number of tanks (make this 100); and\nn for the number of tanks captured (set this to 5)\n\n\n\n# code for part (a)\n\n\nWrite a function called sim_tanks that takes in two arguments: the true number of tanks and the number of tanks captured. The function should simulate the German tank scenario by randomly sampling the tanks without replacement. The function should return a vector of length three of the estimates under \\(\\hat{N}_{1}\\), \\(\\hat{N}_{2}\\), and \\(\\hat{N}_{3}\\), as defined on the homework.\n\n\n# code for part (b)\n\n\nUsing the replicate() function, simulate the German Tank data for n_sim iterations. Then use the t() function to take the transpose of the output such that you have an object that has 3 columns and n_sim rows. Be sure to store your output into a variable!\n\nIn the Console, take a look at the output. Make sure you understand what each row/column represents!\n\nset.seed(311)\n# code for part (c)\n\n\nUsing your simulations from (c), obtain the empirical mean, variance, and MSE of \\(\\hat{N}_{1}\\), \\(\\hat{N}_{2}\\), and \\(\\hat{N}_{3}\\). You can access a specific column \\(j\\) of matrix \\(M\\) using the following code: M[,j]. Similarly, to access a specific row \\(j\\): M[j,]. Comment on how the values compare across the estimates. Based on what you’ve found, which estimator(s) appear to be unbiased? Which estimator(s) appear to be “best”?\n\nNote: do not use the empirical mean and variances to obtain the empirical MSE. Approximate the MSE of an estimator \\(\\delta(X)\\) from its definition of \\(E[(\\delta(X) - \\theta)^2]\\).\n\n# means\n\n\n# variances\n\n\n# MSEs\n\nSolution:\n\nLastly, we will visualize the simulations of the estimates. Run the line code of code where we create df and then take a look at what df looks like by typing View(df) in the Console. Then fill in the remaining code and then set eval = TRUE before knitting. Briefly comment on what you notice.\n\nNote: if you have never installed tidyverse, you will have to do so now. In the CONSOLE at the bottom, paste in and run the the following code: install.packages(\"tidyverse\"). The installation may take a few minutes, but then you should be good to go!\n\nlibrary(tidyverse)\n\ndf <- data.frame(_____) %>% # variable name of your transposed output from (c)\n  rename(\"N1\" = 1, \"N2\" = 2, \"N3\" = 3) %>%\n  pivot_longer(cols = 1:3, names_to = \"estimator\", values_to = \"estimate\") \n\ndf %>%\n  ggplot(., aes(x = ______, # name of variable in df that represents the values of the estimates\n                fill = _____ ))+ # name of variable in df that represents which of the N_i\n  geom_histogram(alpha = 0.5, binwidth = 2) +\n  geom_vline(xintercept = ____ ) # variable representing the true total number of tanks\n\nSolution:"
  },
  {
    "objectID": "weeks/class/EM.html#e-step",
    "href": "weeks/class/EM.html#e-step",
    "title": "EM: Mixture of Normals",
    "section": "E-step",
    "text": "E-step\nWe will write a function called e.step that will calculate \\(P(Y_{i} = j | x_{i}, \\boldsymbol{\\theta} = \\boldsymbol{\\theta}^{old})\\) for given values mu and p.\n\n# x = observed data\n# n = number of observations\n# m = number of mixture components\n# mu = m-vector of means mu\n# p = m-vector of component proportions\ne.step <- function(x, n, m, mu, p){\n  tau_mat <- matrix(NA, nrow = n, ncol = m)\n  for(i in 1:n){\n    p_yij <- dnorm(x[i], mu_old, 1) * p_old\n    tau_mat[i,] <- p_yij/sum(p_yij)\n  }\n  return(tau_mat)\n}"
  },
  {
    "objectID": "weeks/class/EM.html#run-em-algorithm",
    "href": "weeks/class/EM.html#run-em-algorithm",
    "title": "EM: Mixture of Normals",
    "section": "Run EM algorithm",
    "text": "Run EM algorithm\n\n# set m and n\nn <- length(x)\nm <- 3\n\n# number of iterations until convergence; depends on the problem!\nn_sim <- 50\n\n# initialize\nmu_old <- c(0, 5, 10)\np_old <- rep(1/m, m)\n\n# create matrices for storing iterations\nmu_store <- matrix(NA, nrow = n_sim, ncol = m)\np_store <- matrix(NA, nrow = n_sim, ncol = m)\nmu_store[1,] <- mu_old\np_store[1,] <- p_old\n\n# run algorithm\nfor(s in 2:n_sim){\n  # E-step\n  tau_mat <- e.step(x, n, m, mu_old, p_old)\n  \n  # M-step and update in one go\n  for(j in 1:m){\n    mu_old[j] <- sum(x * tau_mat[,j])/sum(tau_mat[,j])\n    p_old[j] <- sum(tau_mat[,j])/n\n  }\n  \n  # just to keep track to monitor convergence\n  mu_store[s,] <- mu_old\n  p_store[s,] <- p_old\n}\n\n\nAssess convergence\nWe will look at the last few iterations to see if the algorithm as converged. If not, we need to increase the number of simulations!\n\n# tail() shows last six elements of a vector/matrix\ntail(mu_store)\n\n          [,1]     [,2]     [,3]\n[45,] 1.564003 4.764206 7.078663\n[46,] 1.563962 4.764083 7.078418\n[47,] 1.563926 4.763974 7.078202\n[48,] 1.563893 4.763878 7.078011\n[49,] 1.563865 4.763792 7.077841\n[50,] 1.563840 4.763717 7.077692\n\ntail(p_store)\n\n           [,1]      [,2]      [,3]\n[45,] 0.2609560 0.5555167 0.1835273\n[46,] 0.2609480 0.5554821 0.1835699\n[47,] 0.2609410 0.5554515 0.1836075\n[48,] 0.2609348 0.5554244 0.1836408\n[49,] 0.2609293 0.5554004 0.1836703\n[50,] 0.2609244 0.5553792 0.1836964\n\n\n\n\n\n\n\n\nExpand to see true values\n\n\n\n\n\nThe true parameter values are:\n\\(\\boldsymbol{\\mu}\\): (2, 5, 7)\n\\(\\mathbf{p}\\): (0.3, 0.5, 0.2)"
  },
  {
    "objectID": "weeks/class/EM_solns.html",
    "href": "weeks/class/EM_solns.html",
    "title": "EM: Mixture of Normals",
    "section": "",
    "text": "library(tidyverse)\n# change the file path if needed\nx <- readRDS(\"~/Downloads/normal_mixture_data.Rda\")\ndata.frame(x = x) %>%\n  ggplot(., aes(x= x))+\n  geom_histogram(bins = 15)"
  },
  {
    "objectID": "weeks/class/EM_solns.html#e-step",
    "href": "weeks/class/EM_solns.html#e-step",
    "title": "EM: Mixture of Normals",
    "section": "E-step",
    "text": "E-step\nWe will write a function called e.step that will calculate \\(P(Y_{i} = j | x_{i}, \\boldsymbol{\\theta} = \\boldsymbol{\\theta}^{old})\\) for given values mu and p. When we call this function, we will pass in \\(\\boldsymbol{\\mu}^{old}\\) and \\(\\mathbf{p}^{old}\\) for mu and p.\n\n# x = observed data\n# n = number of observations\n# m = number of mixture components\n# mu = m-vector of means mu\n# p = m-vector of component proportions\ne.step <- function(x, n, m, mu, p){\n  tau_mat <- matrix(NA, nrow = n, ncol = m)\n  for(i in 1:n){\n    p_yij <- dnorm(x[i], mu, 1) * p\n    tau_mat[i,] <- p_yij/sum(p_yij)\n  }\n  return(tau_mat)\n}"
  },
  {
    "objectID": "weeks/class/EM_solns.html#run-em-algorithm",
    "href": "weeks/class/EM_solns.html#run-em-algorithm",
    "title": "EM: Mixture of Normals",
    "section": "Run EM algorithm",
    "text": "Run EM algorithm\n\n# set m and n\nn <- length(x)\nm <- 3\n\n# number of iterations until convergence; depends on the problem!\nn_sim <- 108\n\n# initialize\nmu_old <- c(0, 5, 10)\np_old <- rep(1/m, m)\n\n# create matrices for storing iterations\nmu_store <- matrix(NA, nrow = n_sim, ncol = m)\np_store <- matrix(NA, nrow = n_sim, ncol = m)\nmu_store[1,] <- mu_old\np_store[1,] <- p_old\n\n# run algorithm\nfor(s in 2:n_sim){\n  # E-step\n  tau_mat <- e.step(x, n, m, mu_old, p_old)\n  \n  # M-step and update in one go\n  for(j in 1:m){\n    mu_old[j] <- sum(x * tau_mat[,j])/sum(tau_mat[,j])\n    p_old[j] <- sum(tau_mat[,j])/n\n  }\n  \n  # just to keep track to monitor convergence\n  mu_store[s,] <- mu_old\n  p_store[s,] <- p_old\n}\n\n\nAssess convergence\nWe will look at the last few iterations to see if the algorithm as converged. If not, we need to increase the number of simulations!\n\n# tail() shows last six elements of a vector/matrix\ntail(mu_store)\n\n           [,1]     [,2]     [,3]\n[103,] 1.563647 4.763139 7.076542\n[104,] 1.563647 4.763138 7.076542\n[105,] 1.563647 4.763138 7.076541\n[106,] 1.563647 4.763138 7.076541\n[107,] 1.563647 4.763138 7.076541\n[108,] 1.563647 4.763138 7.076541\n\ntail(p_store)\n\n           [,1]      [,2]      [,3]\n[103,] 0.260887 0.5552163 0.1838967\n[104,] 0.260887 0.5552163 0.1838967\n[105,] 0.260887 0.5552162 0.1838967\n[106,] 0.260887 0.5552162 0.1838968\n[107,] 0.260887 0.5552162 0.1838968\n[108,] 0.260887 0.5552162 0.1838968\n\n\nIt seems like we hit convergence after 106 iterations. Depending on the problem, we may need fewer or more iterations.\nOur MLEs are \\(\\hat{\\boldsymbol{\\mu}}_{MLE} =\\) (1.5636468, 4.7631381, 7.076541) and \\(\\hat{\\mathbf{p}}_{MLE} =\\) (0.260887, 0.5552162, 0.1838968). How did we do?\n\n\n\n\n\n\nExpand to see true values\n\n\n\n\n\nThe true parameter values are:\n\\(\\boldsymbol{\\mu}\\): (2, 5, 7)\n\\(\\mathbf{p}\\): (0.3, 0.5, 0.2)"
  },
  {
    "objectID": "weeks/class/templates/EM_normal_mixture.html",
    "href": "weeks/class/templates/EM_normal_mixture.html",
    "title": "EM: Mixture of Normals",
    "section": "",
    "text": "library(tidyverse)\n# change the file path if needed\nx <- readRDS(\"~/Downloads/normal_mixture_data.Rda\")\ndata.frame(x = x) %>%\n  ggplot(., aes(x= x))+\n  geom_histogram(bins = 15)"
  },
  {
    "objectID": "weeks/class/templates/EM_normal_mixture.html#create-function-for-e-step",
    "href": "weeks/class/templates/EM_normal_mixture.html#create-function-for-e-step",
    "title": "EM: Mixture of Normals",
    "section": "Create function for E-Step",
    "text": "Create function for E-Step\n\n# x = observed data\n# n = number of observations\n# m = number of mixture components\n# mu = m-vector of means mu\n# p = m-vector of component proportions\ne.step <- function(x, n, m, mu, p){\n \n}"
  },
  {
    "objectID": "weeks/class/templates/EM_normal_mixture.html#em-algorithm",
    "href": "weeks/class/templates/EM_normal_mixture.html#em-algorithm",
    "title": "EM: Mixture of Normals",
    "section": "EM algorithm",
    "text": "EM algorithm\n\n# set m and n\n\n\n# number of iterations until convergence; depends on the problem!\n\n# initialize\n\n\n# create matrices for storing iterations\n\n# Run algorithm\n\nfor(s in 2:___){\n  # E-step\n  \n  # M-step and update in one go\n\n  \n  # store to monitor convergence\n\n  \n}\n\n\nAsses convergence\n\n# assess convergence by looking at the last few iterations"
  },
  {
    "objectID": "weeks/hw/templates/EM_multinom.html",
    "href": "weeks/hw/templates/EM_multinom.html",
    "title": "EM algorithm: Extra Credit",
    "section": "",
    "text": "Be sure to set eval = TRUE before submitting!"
  },
  {
    "objectID": "weeks/hw/templates/EM_multinom.html#e-step-function",
    "href": "weeks/hw/templates/EM_multinom.html#e-step-function",
    "title": "EM algorithm: Extra Credit",
    "section": "E-step function",
    "text": "E-step function\nFill in the remainder of the e_step function which takes in the observed phenotype counts x (vector of length 3) and allele frequencies p (vector of length 3). Using x and p, the function should return a vector of length six of the expected genotype count for each of the six genotypes.\n\n# x = observed phenotype counts (carbonaria, insularia, typica)\n# p = allele probabilities (carbonaria, insularia, typica)\n\ne_step <- function(x, p){\n  n_cc <- \n  n_ci <- \n  n_ct <- \n  n_ii <-\n  n_it <- \n  n_tt <-\n  return()\n}"
  },
  {
    "objectID": "weeks/hw/templates/EM_multinom.html#m-step-function",
    "href": "weeks/hw/templates/EM_multinom.html#m-step-function",
    "title": "EM algorithm: Extra Credit",
    "section": "M-step function",
    "text": "M-step function\nFill in the remainder of the m_step function which takes in the observed phenotype counts x (vector of length 3) and expected genotype counts n (vector of length 6). This function should return a vector of length three of the new estimate of \\(\\boldsymbol{\\theta}\\).\n\n# x = observed phenotype counts (carbonaria, insularia, typica)\n# n = expected genotype counts (CC, CI, CT, II, IT, TT)\nm_step <- function(x, n){\n  p_c <- \n  p_i <- \n  p_t <- \n  return()\n}"
  },
  {
    "objectID": "weeks/hw/templates/EM_multinom.html#implementation",
    "href": "weeks/hw/templates/EM_multinom.html#implementation",
    "title": "EM algorithm: Extra Credit",
    "section": "Implementation",
    "text": "Implementation\nWe have the following observed counts/data of the three phenotypes for the peppered moths: \\(X = (N_{C} = 85, N_{I} = 196, N_{T} = 341)\\).\n\nx <- c(85, 196, 341)\n\nNow to implement the algorithm, complete the following:\n\nCreate a vector p_old of length three that initializes the algorithm with the three alleles being equal in frequency.\nCreate a variable n_sim that sets the number of iterations you’d like to run the algorithm for. I recommend you start with something relatively small just to test your code. You can always modify this value later on!\nCreate a matrix p_mat that will hold/store the \\(\\hat{\\mathbf{p}}\\) estimates from each iteration of the algorithm. Use the matrix() function to do this (think about what dimensions you’ll need p_mat to have). You can fill the matrix with NA or 0 values.\nReplace the first row of p_mat with your initial value/guess for \\(\\mathbf{p}\\).\nComplete the for loop where you iterate between the E and M steps by calling the appropriate functions and passing in the appropriate arguments. You may need to create an additional variable or two.\n\nThen go ahead and run this code chunk (make sure you’ve run all of the previous chunks first).\n\n# 1. Initialize\n\n# 2. Set number of iterations\n\n# 3. Create p_mat\n\n# 4. Replace first row of p_mat\n\n# 5. for loop to run algorithm\n\nfor(i in 2:n_sim){\n  # e_step\n  \n  # m_step\n  \n  # store estimate in p_mat\n\n}\n\n\nAssess convergence and state results.\n\nRun your algorithm until convergence. Roughly how many iterations did we need until we hit convergence?\n\nSolution:\n\nWhat are your maximum likelihood estimates for the frequencies of the three alleles \\(C\\), \\(I\\), and \\(T\\) under the assumption of Hardy-Weinberg?\n\nSolution:\n\nOptional: what are the maximum likelihood estimate for the frequencies of the three alleles \\(C\\), \\(I\\), and \\(T\\) if we don’t assume Hardy-Weinberg?\n\nSolution:"
  },
  {
    "objectID": "weeks/week-08.html",
    "href": "weeks/week-08.html",
    "title": "Week 08",
    "section": "",
    "text": "Tuesday (04/09)\n\nNote: Homework 6 due tomorrow 4/10!\nTopics\n\nFinish up CIs\nEmpirical CDF\nThe bootstrap\n\nDaily assignment\n\nDaily assignment\n\nClass activity\n\nCode from bootstrap distribution demo\n\n\n\nThursday (04/11)\n\nBring your laptops!\nTopics\n\nBootstrap distribution\nBootstrap confidence intervals\n\nDaily assignment\n\nNone!\n\n\n\nFriday (04/12)\n\nBring your laptops!\nTopics\n\nBootstrap confidence intervals (cont.)\n\nDaily assignment\n\nNone!"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Final project",
    "section": "",
    "text": "Downloadable PDF with final project description here.\nKey dates:\n\nSaturday 04/27 at 12:00pm: project proposal due\nSaturday 05/11 at 12:00pm: rough draft due\nSaturday 05/18 at 9:00am: project presentations during exam period\nMonday 05/20 at 11:59pm: final draft and reflection due"
  },
  {
    "objectID": "weeks/hw/templates/hw6_r.html",
    "href": "weeks/hw/templates/hw6_r.html",
    "title": "STAT 311: Homework 6 (R)",
    "section": "",
    "text": "General note: the functions sd() and var() in R calculate the values \\(s\\) and \\(s^2\\) as we’ve seen in class. That is, var(x) calculates \\(s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_{i} - \\bar{x})^2\\), and sd(x) calculates the square root of this quantity."
  },
  {
    "objectID": "weeks/hw/templates/hw6_r.html#problem-1",
    "href": "weeks/hw/templates/hw6_r.html#problem-1",
    "title": "STAT 311: Homework 6 (R)",
    "section": "Problem 1",
    "text": "Problem 1\nIn a 2007 paper, researchers studied whether or not a new drug, K11777, works to treat schistosomiasis. Schistosomiasis is a disease caused by parasitic flatworms so K11777 helps to stop the worms from growing. In the study, 20 mice were infected with schistosomiasis and then half were randomly selected to receive the K11777 treatment. At the end of the treatment, the number of worms in the liver of each rat was counted. We want to construct interval estimates that can answer the question: is the drug helping treat the disease, on average? The data on the number of worms in each of 20 rats is given below.\n\n# rats that received K11777\ntreat <- c(1,2,2,10,7,3,5,9,10,6)\n\n# rats that did not receive K11777\ncontrol <- c(16, 10,10,7, 17, 31, 26, 28, 13, 47)\n\n\nConstruct histograms of the data, one for each group. Do the data appear Normally distributed? Briefly justify your answer.\n\n\n\n\n**Solution:** \n\nNo matter how you answered in (a), we will make the assumption that the data are Normal. (You’ll find that statisticians often default to Normality in the first analysis…) Write a function that takes in two arguments: a vector of data and a coefficient level. The function should return a vector of length two: the lower and upper bounds of the symmetric \\(\\gamma\\)-coefficient confidence interval that is appropriate for this data. Using your function, report a 90% confidence interval for the true average number of parasitic worms in the treatment group.\n\n\n\n\n\nUsing your same function, construct and report a 90% confidence interval for the true average number of parasitic worms in the control group.\n\n\n\n\n\nBased on your intervals in (b) and (c), do you think the drug is helping? Briefly explain why or why not. (It might help to interpret the intervals first)\nSolution:"
  },
  {
    "objectID": "weeks/hw/templates/hw6_r.html#problem-2",
    "href": "weeks/hw/templates/hw6_r.html#problem-2",
    "title": "STAT 311: Homework 6 (R)",
    "section": "Problem 2",
    "text": "Problem 2\nWe saw in class that the coverage probability of a random interval \\((A(\\mathbf{X}), B(\\mathbf{X}))\\) is the probability that the interval contains \\(\\theta\\). In order for \\((A(\\mathbf{X}), B(\\mathbf{X}))\\) to be a \\(100\\times \\gamma\\) % confidence interval for \\(\\theta\\), it must have coverage probability of at least \\(\\gamma\\) for all \\(\\theta \\in \\Omega\\). We will investigate coverage rates for a variety of random intervals in this problem.\n\nSuppose \\(X_{1},\\ldots, X_{n}\\) are an IID random sample for \\(N(0,1)\\). Use R to simulate nsim = 10000 sample data sets (consider making this a variable), each of size \\(n=5\\). Make it such that your resulting matrix has 10000 rows and 5 columns moving forward.\n\n\n\n\n\nFor each sample from (a), find and store the bounds of a symmetric 95% confidence interval for \\(\\mu\\). Can you make use of a function from a previous problem? DO NOT REPORT/PRINT/DISPLAY ALL 10000 INTERVALS IN YOUR FINAL SUBMISSION.\nNote: while you used the true values of \\(\\mu\\) and \\(\\sigma^2\\) to simulate the datasets in (a), you should treat \\(\\mu\\) and \\(\\sigma^2\\) as unknown for this part.\n\n\n\n\n\nEstimate and report the coverage rate of this confidence interval procedure by computing the proportion of samples which produced a 95% confidence interval that contained the true population mean \\(\\mu\\).\n\n\n\n\n\nNow suppose we compute quantiles for our confidence interval for \\(\\mu\\) using the Normal distribution. This would correspond to incorrectly treating the observed sample standard deviation \\(s\\) as the true value of the population standard deviation \\(\\sigma\\). If we make this assumption, the confidence interval looks almost exactly like the one in Exercise 8.5.1 (and also seen in Friday’s class), where we’ve simply replaced \\(\\sigma\\) with \\(s\\): \\[\\left(\\bar{X} - \\Phi^{-1}\\left(\\frac{1+\\gamma}{2}\\right) \\frac{s}{\\sqrt{n}}, \\bar{X} + \\Phi^{-1}\\left(\\frac{1+\\gamma}{2}\\right) \\frac{s}{\\sqrt{n}} \\right)\\] where \\(\\Phi^{-1}\\) is the quantile function for the standard Normal.\nNow repeat part (b), but this time using the confidence interval formula above.\n\n\n\n\n\nEstimate and report the coverage rate of the confidence interval procedure in (d) by computing the proportion of samples which produced a 95% confidence intervals that contained the true population mean \\(\\mu\\). How do the coverage rates from the procedures in (b) and (d) compare?\n\n\n\n\n**Solution:**"
  },
  {
    "objectID": "weeks/week-09.html",
    "href": "weeks/week-09.html",
    "title": "Week 09",
    "section": "",
    "text": "Tuesday (04/16)\n\nTopics\n\nIntroduction to Hypothesis tests\nPower\nSignificance level\n\nDaily assignment\n\nTBD\n\n\n\nThursday (04/18)\n\nTopics\n\nSize of test\np-values\n\nDaily assignment\n\nTBD\n\n\n\nFriday (04/19)\n\nTopics\n\nNO CLASS: Spring symposium\n\nDaily assignment\n\nNone! But it’s a good idea to skim over readings and notes from this week."
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10",
    "section": "",
    "text": "Tuesday (04/23)\n\nTopics\n\nP-value (cont.)\nEquivalence of Confidence Sets and Hypothesis Tests\n\nDaily assignment\n\nTBD\n\n\n\nThursday (04/25)\n\nTopics\n\nLikelihood ratio test\n\nDaily assignment\n\nTBD\n\n\n\nFriday (04/26)\n\nProject proposals due tomorrow at 12:00pm to Canvas!\nTopics\n\n\\(t\\)-test\n\nDaily assignment\n\nNone!"
  },
  {
    "objectID": "weeks/week-11.html",
    "href": "weeks/week-11.html",
    "title": "Week 11",
    "section": "",
    "text": "Tuesday (04/30)\n\nTopics\n\nTwo-sample \\(t\\)-test\n\\(F\\)-distribution\n\nDaily assignment\n\nTBD\n\n\n\nThursday (05/02)\n\nTopics\n\n\\(F\\)-test\n\nDaily assignment\n\nTBD\n\n\n\nFriday (05/03)\n\nMidterm 2 today!\nTopics\n\nReview day\n\nDaily assignment\n\nNone!"
  },
  {
    "objectID": "weeks/week-12.html",
    "href": "weeks/week-12.html",
    "title": "Week 12",
    "section": "",
    "text": "Tuesday (05/07)\n\nTopics\n\nMethod of least squares\nSLR\n\nDaily assignment\n\nTBD\n\n\n\nThursday (05/09)\n\nTopics\n\nInference for SLR\nCRFs\n\nDaily assignment\n\nTBD\n\n\n\nFriday (05/11)\n\nTopics\n\nInference for SLR (cont.)\n\nDaily assignment\n\nNone!"
  },
  {
    "objectID": "weeks/class/bootstrap_dist.html",
    "href": "weeks/class/bootstrap_dist.html",
    "title": "Empirical Bootstrap Distribution",
    "section": "",
    "text": "Consider the following sample of eruption times (in seconds) of the Old Faithful geyser:\n\n# obtain data\ndata(\"faithful\")\nx <- faithful$eruptions * 60\nhist(x)\n\n\n\n\nThis data is certainly not Normal!\nPerhaps we’d like to obtain a bootstrap distribution for the median eruption time.\n\nn <- length(x)\nB <- 5000\ndelta_star <- rep(NA, B)\nfor(i in 1:B){\n  xstar <- sample(x, size = n, replace = T)\n  delta_star[i] <- median(xstar) \n}\n\n# bootstrap distribution of medians\nhist(delta_star)\n\n\n\n# bootstrap estimate of bias\nmean(delta_star) - median(x)\n\n[1] -0.90333"
  },
  {
    "objectID": "weeks/class/bootstrap_dist.html#example-2-bootstrapping-for-mse",
    "href": "weeks/class/bootstrap_dist.html#example-2-bootstrapping-for-mse",
    "title": "Empirical Bootstrap Distribution",
    "section": "Example 2: bootstrapping for MSE",
    "text": "Example 2: bootstrapping for MSE\nThe coefficient of variation of a distribution is the quantity \\(\\frac{\\sigma}{\\mu}\\), where \\(\\sigma\\) and \\(\\mu\\) are the standard deviation and mean of the distribution, respectively. Let us generate some Poisson data and obtain estimates of the mean squared error (MSE) of the estimator \\(\\frac{s}{\\bar{x}}\\) where \\(s\\) is the sample standard deviation and \\(\\bar{x}\\) is the sample mean.\nNotice that in the following, I am setting a seed using the set.seed() function. The input doesn’t really matter. What this function does is ensure that the random generator in R generates the same sequence of random values. That way when we knit or run the same code that involves random sampling on different laptops, we will get the same result.\n\n# generate some Poisson data\nset.seed(309)\nlambda_true <- 1\nn <- 20\nx <- rpois(n, lambda_true)\n\nAn estimate for coefficient variation from the data is:\n\ncv_hat <- sd(x)/mean(x)\ncv_hat\n\n[1] 1.00006\n\n\n\nBootstrap estimate\n\nB <- 5000\ndelta_star <- rep(NA, B)\nfor(i in 1:B){\n  # if size isn't specified in sample(), defaults to length of x!\n  xstar <- sample(x, replace = T)\n  delta_star[i] <- sd(xstar)/mean(xstar)\n}\nhist(delta_star)\n\n\n\n# bootstrap estimate of MSE\nmean((delta_star - cv_hat)^2)\n\n[1] 0.03948413\n\n\n\n\n“Monte Carlo” estimate\nSince we know the true distribution, we don’t need to bootstrap to estimate the MSE of \\(\\frac{s}{\\bar{x}}\\) for \\(\\frac{\\sigma}{\\mu}\\). That is, rather than resampling from the original observation x, we can actually simulate new data sets from the Poisson distribution. Then we can compare the estimates to the true coefficient of variation, which for a \\(\\text{Poisson}(\\lambda)\\) distribution is \\(\\frac{\\sqrt{\\lambda}}{\\lambda}\\).\n\ndelta_hat <- rep(NA, B)\nfor(i in 1:B){\n  x_new <- rpois(n, lambda_true)\n  delta_hat[i] <- sd(x_new)/mean(x_new)\n}\nhist(delta_hat)\n\n\n\ncv_true <- sqrt(lambda_true)/lambda_true\nmean((delta_hat - cv_true)^2)\n\n[1] 0.04043435"
  },
  {
    "objectID": "weeks/class/bootstrap_ci.html",
    "href": "weeks/class/bootstrap_ci.html",
    "title": "Bootstrap confidence intervals",
    "section": "",
    "text": "data(\"faithful\")\nx <- faithful$eruptions * 60\nn <- length(x)\nsamp_med <- median(x)\nB <- 5000\ndelta_star <- rep(NA, B)\nfor(i in 1:B){\n  xstar <- sample(x, size = n, replace = T)\n  delta_star[i] <- median(xstar) - samp_med\n}\n\n# bootstrap distribution of delta\nhist(delta_star)\n\n\n\n# (approximate) bootstrap CI\nci <- samp_med - quantile(delta_star, c(0.975, 0.025))\nnames(ci) <- c(\"2.5\", \"97.5\")\nci\n\n   2.5   97.5 \n233.49 250.02 \n\n\n233.49, 250.02\nThe approximate 95% bootstrap CI for the median eruption time of Old Faithul is [233.49, 250.02].\n\n\n\n\nx <- c(rep(\"AA\", 342), rep(\"Aa\", 500), rep(\"aa\", 187))\nn <- length(x)\nB <- 1000\ntheta_hat <- (2*sum(x == \"AA\" ) + sum(x == \"Aa\"))/(2*n)\n\ndelta_star <- rep(NA, B)\nfor(i in 1:B){\n  x_star <- sample(x, size = n, replace = T)\n  delta_star[i] <- (2*sum(x_star == \"AA\" ) + sum(x_star == \"Aa\"))/(2*n) - theta_hat\n}\n\ntheta_hat - quantile(delta_star, c(0.975, 0.025))\n\n    97.5%      2.5% \n0.5534500 0.5962099"
  },
  {
    "objectID": "weeks/class/bootstrap_ci.html#parametric",
    "href": "weeks/class/bootstrap_ci.html#parametric",
    "title": "Bootstrap confidence intervals",
    "section": "Parametric",
    "text": "Parametric\nReturning to the Old Faithful example: suppose we assume that the eruption times are Normal (even though the data clearly show that they are not…). For a parametric bootstrap, we will take repeated samples from a Normal distribution with mean and variance estimated from the observed data, then proceed as we did in the nonparametric bootstrap:\n\nx <- faithful$eruptions * 60\nn <- length(x)\nsamp_mean <- mean(x)\nsamp_sd_mle <- sd(x)*sqrt((n-1)/n)\nsamp_med <- median(x)\nB <- 5000\ndelta_star <- rep(NA, B)\nfor(i in 1:B){\n  xstar <- rnorm(n, samp_mean, samp_sd_mle)\n  delta_star[i] <- median(xstar) - samp_med\n}\nhist(delta_star)\n\n\n\n# (approximate) bootstrap CI\nci <- samp_med - quantile(delta_star, c(0.975, 0.025))\nnames(ci) <- c(\"2.5\", \"97.5\")\nci\n\n     2.5     97.5 \n260.2404 280.8233"
  }
]